[
  {
    "objectID": "programming.html",
    "href": "programming.html",
    "title": "Posts in Programming",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nConvolution, Contiguous Access, and Vectorization\n\n\nSep 21, 2025\n\n\n\n\n\n\nNewton’s Root-Finding Method\n\n\nSep 17, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/convolution-contiguous-access-vectorization.html",
    "href": "posts/convolution-contiguous-access-vectorization.html",
    "title": "Convolution, Contiguous Access, and Vectorization",
    "section": "",
    "text": "In programming, it is well understood that accessing memory contiguously is faster than strided or random access. The reason for this is that data is transferred in contiguous chunks, or cache lines, between main memory, CPU cache, and registers. This is why image-processing algorithms typically place the horizontal axis in the inner loop: pixels along that axis are adjacent in memory.\nMany image-processing algorithms use convolution, where a kernel is applied to each pixel by iterating over its neighborhood. For a 2D image and kernel, this requires four nested for-loops. This is the textbook order:\nfor y in image:\n    for x in image row:\n        for v in kernel:\n            for u in kernel row:\n                output[x, y] += kernel[u, v] * image[x + u, y + v]\nThis is obviously better than first iterating over x and then over y. But we can do better! Since the image is generally larger than the kernel, it is more efficient to place the image’s x-axis in the innermost for-loop:\nfor y in image:\n    for v in kernel:\n        for u in kernel row:\n            for x in image row:\n                output[x, y] += kernel[u, v] * image[x + u, y + v]\nThis ordering improves contiguous read/write access and makes better use of the (prefetched) cache lines. But there’s another (and perhaps bigger) advantage to this ordering: it allows for efficient vectorization.\nModern CPU’s can do vector-operations, which means a single core can do e.g. 4 or 8 additions, multiplications or other simple instructions at once. Note that vectorization (a Single Instruction Mulitple Data technique) is different from multi-core parallelization (Multiple Instruction Multiple Data). Both these techniques can complement eachother.\nMany compilers have an auto-vectorization option which is turned off by default, because it is not compatible with older CPU’s. In GCC, you can turn it on using the -ftree-vectorize flag. The AVX instruction set is enabled using the -mavx flag.\nIn the tables below I listed the processing times for different orderings and compiler options. Without vectorization, the more contiguous {y, v, u, x} ordering is 1.5x as fast as the default {y, x, v, u} ordering. With vectorization, it is 3x (SSE) to 8x (AVX) as fast. That’s considerable! It can also be seen that a transposed {x, y, v, u} ordering is just a little slower when the image is small (1 MiB) and fits in the CPU cache. But when the image is larger (16 MiB), cache misses due to strided iteration become prominent.\n\nProcessing time in milliseconds for a small 3×3 kernel and small 512×512 image.\n\n\nVectorization\nx,y,v,u\ny,x,v,u\ny,v,u,x\n\n\n\n\nNone\n2.8\n1.6\n1.1\n\n\nSSE\n2.9\n1.6\n0.5\n\n\nAVX\n2.9\n1.9\n0.2\n\n\n\n\nProcessing time in milliseconds for a large 11×11 kernel and small 512×512 image.\n\n\nVectorization\nx,y,v,u\ny,x,v,u\ny,v,u,x\n\n\n\n\nNone\n20.8\n20.3\n14.1\n\n\nSSE\n20.8\n20.3\n5.8\n\n\nAVX\n21.2\n21.6\n2.6\n\n\n\n\nProcessing time in milliseconds for a small 3×3 kernel and large 512×8192 image.\n\n\nVectorization\nx,y,v,u\ny,x,v,u\ny,v,u,x\n\n\n\n\nNone\n131.1\n26.0\n17.4\n\n\nSSE\n127.8\n26.2\n8.5\n\n\nAVX\n128.8\n31.3\n3.6\n\n\n\nThis is the C-code that was used to measure these timings.\n\n\nCode\n\n// Convolutions with different memory access patterns.\n// Compile with: gcc -O2 -ftree-vectorize -mavx -std=c99 -o convolution .\\convolution.c\n\n// Copyright by Edwin Bennink\n\n#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;time.h&gt;\n\n\n#define MIN(x, y) (((x) &lt; (y)) ? (x) : (y))\n#define MAX(x, y) (((x) &gt; (y)) ? (x) : (y))\n\n\n// Convolves the image with the kernel. Uses the transposed order of iteration: {x, y, v, u}, in\n// which (x, y) are the image coordinates and (u, v) are the kernel coordinates. This is a ordering,\n// because of strided memory access.\n// The kernel is truncated at the image boundaries.\nvoid convolve_transposed(const float* image, float * output, const int width, const int height,\n                         const float* kernel, const int radius) {\n    // Iterate over the columns and rows in the image.\n    for (int x = 0; x &lt; width; x++) {\n        for (int y = 0; y &lt; height; y++) {\n            const int i = y * width + x;  // Index into the output image.\n            \n            // Initialize the output image with zeros.\n            output[i] = 0.0f;\n            \n            // Iterate over the rows in the kernel. Truncate the iteration at the top and bottom\n            // image border.\n            for (int v = MAX(0, radius - y); v &lt; MIN(2*radius + 1, radius + height - y); v++) {\n                const int k = v * (2*radius + 1);  // Index into the kernel.\n                const int j = (y + v - radius) * width + x - radius;  // Index into the input image.\n                \n                // Iterate over the columns in the kernel. Truncate the iteration at the left and\n                // right image border.\n                for (int u = MAX(0, radius - x); u &lt; MIN(2*radius + 1, radius + width - x); u++) {\n                    // Multiply image with kernel and add to the output.\n                    output[i] += image[j + u] * kernel[k + u];\n                }\n            }\n        }\n    }\n}\n\n\n// Convolves the image with the kernel. Uses the generally accepted order of iteration: {y, x, v, u},\n// in which (x, y) are the image coordinates and (u, v) are the kernel coordinates.\n// The kernel is truncated at the image boundaries.\nvoid convolve_default(const float* image, float * output, const int width, const int height,\n                      const float* kernel, const int radius) {\n    // Iterate over the rows and columns in the image.\n    for (int y = 0; y &lt; height; y++) {\n        for (int x = 0; x &lt; width; x++) {\n            const int i = y * width + x;  // Index into the output image.\n            \n            // Initialize the output image with zeros.\n            output[i] = 0.0f;\n            \n            // Iterate over the rows in the kernel. Truncate the iteration at the top and bottom\n            // image border.\n            for (int v = MAX(0, radius - y); v &lt; MIN(2*radius + 1, radius + height - y); v++) {\n                const int k = v * (2*radius + 1);  // Index into the kernel.\n                const int j = (y + v - radius) * width + x - radius;  // Index into the input image.\n                \n                // Iterate over the columns in the kernel. Truncate the iteration at the left and\n                // right image border.\n                for (int u = MAX(0, radius - x); u &lt; MIN(2*radius + 1, radius + width - x); u++) {\n                    // Multiply image with kernel and add to the output.\n                    output[i] += image[j + u] * kernel[k + u];\n                }\n            }\n        }\n    }\n}\n\n\n// Convolves the image with the kernel. Uses the more contiguous order of iteration: {y, v, u, x},\n// in which (x, y) are the image coordinates and (u, v) are the kernel coordinates. \n// The kernel is truncated at the image boundaries.\nvoid convolve_contiguous(const float* image, float* output, const int width, const int height,\n                     const float* kernel, const int radius) {\n    // Iterate over the rows in the image.\n    for (int y = 0; y &lt; height; y++) {\n        \n        // Initialize the output image with zeros.\n        for (int i = y * width; i &lt; (y + 1) * width; i++) {\n            output[i] = 0.0f;\n        }\n        \n        // Iterate over the rows in the kernel. Truncate the iteration at the top and bottom image\n        // border.\n        for (int v = MAX(0, radius - y); v &lt; MIN(2*radius + 1, radius + height - y); v++) {            \n            // Iterate over the columns in the kernel.\n            for (int u = 0; u &lt; 2*radius + 1; u++) {\n                const int k = v * (2*radius + 1) + u;  // Index into the kernel.\n                int i = y * width;  // Index into the input image.\n                int j = (y + v - radius) * width + u - radius;  // Index into the output image.\n                \n                // Iterate over the columns in the image. Note how this for-loop does contiguous \n                // reads/writes in output and image. It allows for efficient vectorization.\n                // Truncate the iteration at the left and right image border.\n                for (int x = MAX(0, radius - u); x &lt; MIN(width, width - u + radius); x++) {\n                    output[i + x] += image[j + x] * kernel[k];\n                }\n            }\n        }\n    }\n}\n\n\nint main() {\n    const int width = 512;  // Image width\n    const int height = 512;  // Image height\n    const int radius = 1;  // Kernel radius\n    const int kernel_size = (2*radius + 1) * (2*radius + 1);\n    const int n_iterations = 1000;  // Number of convolution iterations\n    clock_t start, end;\n    float duration_ms;\n    FILE *fp; \n\n    // Allocate new image and fill with random values in the range 0-1.\n    float *image = malloc(height * width * sizeof(float *));\n    for (int y = 0; y &lt; height; y++) {\n        for (int x = 0; x &lt; width; x++) {\n            image[y * width + x] = (float)rand() / RAND_MAX;\n        }\n    }\n    \n    // Create a normalized random kernel with a specified radius.\n    double sum = 0;\n    float kernel[kernel_size];\n    for (int i = 0; i &lt; kernel_size; i++) {\n        kernel[i] = (float)rand() / RAND_MAX;\n        sum += kernel[i];\n    }\n    for (int i = 0; i &lt; kernel_size; i++) {\n        kernel[i] /= sum;\n    }\n    \n    // Save the input image.\n    fp = fopen(\"image.raw\", \"wb\");\n    fwrite(image, sizeof(float), width * height, fp);\n    fclose(fp);\n    \n    // Allocate an output image.\n    float *output = malloc(height * width * sizeof(float *));\n    \n    // Apply the convolution operation using the transposed ordering.\n    start = clock();\n    for (int i = 0; i &lt; n_iterations; i++) {\n        convolve_transposed(image, output, width, height, kernel, radius);\n    }\n    end = clock();\n    duration_ms = 1e3f * ((float)(end - start)) / CLOCKS_PER_SEC / n_iterations;\n    printf(\"Transposed: %.2f ms per iteration\\n\", duration_ms);\n    \n    // Write the output to file.\n    fp = fopen(\"transposed.raw\", \"wb\");\n    fwrite(output, sizeof(float), width * height, fp);\n    fclose(fp);\n    \n    // Apply the convolution operation using default ordering.\n    start = clock();\n    for (int i = 0; i &lt; n_iterations; i++) {\n        convolve_default(image, output, width, height, kernel, radius);\n    }\n    end = clock();\n    duration_ms = 1e3f * ((float)(end - start)) / CLOCKS_PER_SEC / n_iterations;\n    printf(\"Default: %.2f ms per iteration\\n\", duration_ms);\n    \n    // Write the output to file.\n    fp = fopen(\"default.raw\", \"wb\");\n    fwrite(output, sizeof(float), width * height, fp);\n    fclose(fp);\n    \n    // Apply the convolution operation using a more contiguous ordering.\n    start = clock();\n    for (int i = 0; i &lt; n_iterations; i++) {\n        convolve_contiguous(image, output, width, height, kernel, radius);\n    }\n    end = clock();\n    duration_ms = 1e3f * ((float)(end - start)) / CLOCKS_PER_SEC / n_iterations;\n    printf(\"Contiguous: %.2f ms per iteration\\n\", duration_ms);\n    \n    // Write the output to file.\n    fp = fopen(\"contiguous.raw\", \"wb\");\n    fwrite(output, sizeof(float), width * height, fp);\n    fclose(fp);\n\n    // Cleanup.\n    free(image);\n    free(output);\n\n    return 0;\n}\n\nKey takeaway: the order of iteration matters, and vectorization brings significant benefits."
  },
  {
    "objectID": "math.html",
    "href": "math.html",
    "title": "Posts in Image Math",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nNewton’s Root-Finding Method\n\n\nSep 17, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "image-analysis.html",
    "href": "image-analysis.html",
    "title": "Posts in Image Analysis",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nConvolution, Contiguous Access, and Vectorization\n\n\nSep 21, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "all.html",
    "href": "all.html",
    "title": "Browse by Topic",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nConvolution, Contiguous Access, and Vectorization\n\n\nSep 21, 2025\n\n\n\n\n\n\nNewton’s Root-Finding Method\n\n\nSep 17, 2025\n\n\n\n\n\n\nQuarto Cheat Sheet\n\n\nSep 15, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EdwinBennink.github.io",
    "section": "",
    "text": "This website shares notes and reflections on (medical) image analysis, programming, and mathematics."
  },
  {
    "objectID": "index.html#about-the-author",
    "href": "index.html#about-the-author",
    "title": "EdwinBennink.github.io",
    "section": "About the author",
    "text": "About the author\nEdwin Bennink works as a researcher and research software engineer for the Medical Physics Group and the Image Sciences Institute (UMC Utrecht - Dept. of Radiology).\nHe received the MSc degree (cum laude) in biomedical engineering from the Eindhoven University of Technology in 2007. In 2016, he received his PhD from the Utrecht University, with a research project on quantitative CT brain perfusion analysis. From 2007 to 2011 he worked as senior software developer for Scientific Volume Imaging BV.\nBesides doing research in CT imaging in acute stroke and supporting various image-analysis related projects throughout the UMC Utrecht, Edwin is one of the lead developers of IMAGR, the image analytics infrastructure that brings deep-learning algorithms into the clinical workflow."
  },
  {
    "objectID": "posts/cheat-sheet.html",
    "href": "posts/cheat-sheet.html",
    "title": "Quarto Cheat Sheet",
    "section": "",
    "text": "This static website is generated using Quarto, which allows creating dynamic content with Python, and writing markdown, including equations, citations, crossrefs, figure panels, etc.\nThis document is used as a Quarto cheat sheet."
  },
  {
    "objectID": "posts/cheat-sheet.html#a-latex-equation",
    "href": "posts/cheat-sheet.html#a-latex-equation",
    "title": "Quarto Cheat Sheet",
    "section": "1. A \\(\\LaTeX\\) Equation",
    "text": "1. A \\(\\LaTeX\\) Equation\n\\[g(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}(x-\\mu)^2/\\sigma^2}\\]"
  },
  {
    "objectID": "posts/cheat-sheet.html#interactive-plotly-plot-with-sliders",
    "href": "posts/cheat-sheet.html#interactive-plotly-plot-with-sliders",
    "title": "Quarto Cheat Sheet",
    "section": "2. Interactive Plotly Plot with Sliders",
    "text": "2. Interactive Plotly Plot with Sliders\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\n\nx = np.arange(0, 10, 0.01)\nn = 51  # Number of traces\nactive = 10  # Initially active slider step\nsteps = []  # Slider steps\n\n# Create a figure and a trace for each slider step.\nfig = go.Figure()\nfor i, f in enumerate(np.linspace(0, 5, n)):\n    trace = go.Scatter(x=x, y=np.sin(f * x), visible=(i == active), name=\"f = \" + str(f))\n    fig.add_trace(trace)\n    \n    steps.append({\n        \"method\": \"update\",\n        \"label\": f\"{f:.2f}\",  # This is the label the slider shows.\n        \"args\": [{\"visible\": [i == j for j in range(n)]}],  # i-th trace is visible in this step.\n    })\n\nsliders = [{\n    \"active\": active,  # Initially active step.\n    \"currentvalue\": {\"prefix\": \"Frequency: \"},  # Slider info text configuration.\n    \"steps\": steps\n}]\n\nfig.update_layout(\n    sliders=sliders,\n    template=\"seaborn\"\n)\nfig.update_yaxes(range=[-1.1, 1.1])  # y-axis are automatically resized otherwise.\n\nfig.show()"
  },
  {
    "objectID": "posts/cheat-sheet.html#interactive-image-hover",
    "href": "posts/cheat-sheet.html#interactive-image-hover",
    "title": "Quarto Cheat Sheet",
    "section": "3. Interactive Image Hover",
    "text": "3. Interactive Image Hover\n\n\nCode\nimport numpy as np\nimport plotly.express as px\n\n# Create a 16x16 RGB image\nimg = np.random.rand(16, 16, 3)\nmagnitude = np.linalg.norm(img, axis=2)\n\nfig = px.imshow(magnitude, color_continuous_scale=\"Viridis\", title=\"Hover me\")\nfig.update_traces(\n    hovertemplate=\"Coord: (%{x}, %{y})&lt;br&gt;Value: (%{customdata[0]:.3f}, %{customdata[1]:.3f}, %{customdata[2]:.3f})&lt;extra&gt;&lt;/extra&gt;\",\n    customdata=img\n)\nfig.show()"
  },
  {
    "objectID": "posts/newtons-method.html",
    "href": "posts/newtons-method.html",
    "title": "Newton’s Root-Finding Method",
    "section": "",
    "text": "At the core of many research software applications you’ll find optimization functions. Sometimes, the optimization task is complex and thus requires a complex optimization routine. In other cases, however, simpler and more elegant methods exist and complex optimization routines might introduce more overhead than necessary.\nIn this post, I’ll show how Newton’s method can be helpful in finding a root of an nth order polynomial, when a close approximation is known. Newton’s method is simple (it requires just a few lines of code) and fast (the rate of convergence is at least quadratic). I recently used it for diffusion MRI software, so I’ll use that as an example.\nIn diffusion MRI, the b-value for trapezoidal gradient pulses is given by \\[b = \\gamma^2G^2\\left(\\delta^2\\left(\\Delta - \\frac{1}{3}\\delta\\right) + \\frac{1}{30}\\xi^2 - \\frac{1}{6}\\delta\\xi^2\\right),\\] where \\(\\xi\\) is the rise time \\(G/s_{max}\\).\nIn order to calculate the gradient magnitude \\(G\\) for a given \\(\\gamma\\), \\(s_{max}\\), \\(b\\), \\(\\delta\\), and \\(\\Delta\\), we need to find the root for \\[0 = \\left(\\frac{\\gamma^2}{30s_{max}^2}\\right)G^5 - \\left(\\frac{\\gamma^2\\delta}{6s_{max}²}\\right)G^4 + \\gamma^2\\delta^2\\left(\\Delta - \\frac{1}{3}\\delta\\right)G^2 - b\\label{eq:1} \\tag{1}\\]\nSince we know that the term for \\(G^5\\) is significantly smaller than the others, we can use the solution to \\[0 = -\\left(\\frac{\\gamma^2\\delta}{6s_{max}²}\\right)\\tilde{G}^4 + \\gamma^2\\delta^2\\left(\\Delta - \\frac{1}{3}\\delta\\right)\\tilde{G}^2 - b\\] as an initial guess. Solving this quadratic equation for \\(\\tilde{G}^2\\) gives \\(\\tilde{G}^2 = (\\sqrt{c_2^2 + 4 c_4 b} - c_2) / (2 c_4)\\), where \\(c_2\\) and \\(c_4\\) are the 2nd and 4th order coefficients in Equation 1.\nNow we have coarse approximation to the value for \\(G\\) for which Equation 1 is zero, we can use Newton’s method to approximate it to the required precision.\nFirst we need a function to evaluate a polynomial given an array of coefficients:\n\nimport numpy as np\n\ndef eval_polynomial(c, x):\n    \"\"\"\n    Evaluates the polynomial y = c[0] + c[1] x + c[2] x² + ...\n\n     :param c: List of coefficients or coefficient vectors.\n     :param x: x\n     :return: y\n    \"\"\"\n    y = 0\n    xp = 1\n    \n    for coefficients in c:\n        y += coefficients * xp  # Add the term c[i] * x^i\n        xp *= x  # Calculate x^(i+1)\n\n    return y\n\nThen we can implement the actual root-finding method like this:\n\ndef newtons_method(c, x0, n):\n    \"\"\"\n    Uses Newton's method to find a root of a polynomial with coefficients c, starting at x = x0:\n     0 = c[0] + c[1] x + c[2] x² + ...\n\n     :param c: List of coefficients or coefficient vectors.\n     :param x0: Initial guess.\n     :param n: Number of iterations.\n     :return: Result of Newton's method after n iterations.\n    \"\"\"\n    # Calculate the coefficients for the derivative of the polynomial.\n    d = [i * coef for i, coef in enumerate(c[1:], 1)]\n\n    x = np.array(x0)  # Makes a copy\n    for _ in range(n):  # Perform n iterations.\n        y = eval_polynomial(c, x)  # Evaluate the polynomial for x.\n        dy = eval_polynomial(d, x)  # Evaluate its derivative for x.\n        nonzero = dy != 0  # Prevent division by zero.\n        x[nonzero] -= y[nonzero] / dy[nonzero]\n\n    return x\n\nNow we can calculate the initial estimate \\(\\tilde{G}\\) and a precise value for \\(G\\) as follows:\n\n# Acquisition parameters for which to calculate G.\nb = 750  # s/mm²\nγ = 267598.7  # 1/mT/s\nδ = 0.002  # s\nΔ = 0.02 # s\ns_max = 50  # mT/mm/s\n\n# Calculate the polynomial coefficients.\nc5 = γ ** 2 / (30 * s_max ** 3)\nc4 = -γ ** 2 * δ / (6 * s_max ** 2)\nc2 = (γ * δ) ** 2 * (Δ - δ / 3)\nc0 = -b\nc = [c0, 0, c2, 0, c4, c5]\n\nx0 = np.sqrt((np.sqrt(c2 ** 2 + 4 * c4 * b) - c2) / (2 * c4))  # Initial estimate for G.\ng = newtons_method(c, x0, 5)  # Precise estimate for G, after 5 iterations.\n\ny0 = eval_polynomial(c, x0)\ny = eval_polynomial(c, g)\nprint(f\"Value at initial estimate G={x0:.6f}: {y0:.3f}\")\nprint(f\"Value at precise estimate G={g:.6f}: {y:.3f}\")\n\nValue at initial estimate G=0.464232: 411.732\nValue at precise estimate G=0.379543: 0.000\n\n\nThe plot below shows the estimate for \\(G\\) in each iteration. Note that the estimate for \\(G\\) is at the intersection of the derivative with \\(y=0\\) in the previous iteration.\n\n\nCode\nimport plotly.graph_objects as go\n\n# Calculate the coefficients for the derivative of the polynomial.\nd = [i * coef for i, coef in enumerate(c[1:], 1)]\n\n\nx = np.linspace(0.35, 0.5, 200)\nn = 6  # Number of iterations\nactive = 0  # Initially active slider step\nsteps = []  # Slider steps\n\nfig = go.Figure()\nfig.add_hline(y=0, line={\"color\": \"black\", \"width\": 1})\n\ntrace = go.Scatter(x=x, y=eval_polynomial(c, x), visible=True, name=\"5th order\")\nfig.add_trace(trace)\ntrace = go.Scatter(x=x, y=eval_polynomial([c0, 0, c2, 0, c4, 0], x), visible=True, name=\"approximation\")\nfig.add_trace(trace)\n    \n# Create a figure and a trace for each slider step.\nfor i in range(n):\n    g = newtons_method(c, [x0], i)\n    y = eval_polynomial(c, g)  # Evaluate the polynomial for g.\n    trace = go.Scatter(mode='markers', marker={\"size\": 8, \"symbol\": \"circle\", \"color\": \"black\"},\n                       x=g, y=y, visible=(i == active), name=\"estimated root\")\n    fig.add_trace(trace)\n    \n    dy = eval_polynomial(d, g)  # Evaluate its derivative for g.\n    trace = go.Scatter(mode='lines', line={\"color\": \"black\", \"width\": 1, \"dash\": \"dash\"},\n                       x=[x[0], x[-1]],\n                       y=[dy[0] * (x[0] - g[0]) + y[0], dy[0] * (x[-1] - g[0]) + y[0]],\n                       visible=(i == active), name=\"derivative at estimation\")\n    fig.add_trace(trace)\n    \n    steps.append({\n        \"method\": \"update\",\n        \"label\": str(i),  # This is the label the slider shows.\n        \"args\": [{\"visible\": [True, True] + [i == j//2 for j in range(2*n)]}],  # i-th trace is visible in this step.\n    })\n\nsliders = [{\n    \"active\": active,  # Initially active step.\n    \"currentvalue\": {\"prefix\": \"Iterations: \"},  # Slider info text configuration.\n    \"steps\": steps\n}]\n\nfig.update_layout(\n    sliders=sliders,\n    template=\"seaborn\",\n    xaxis={\"title\": {\"text\": \"G (mT/mm)\"}},\n    legend={\"x\": 0.02, \"y\": 0.98},\n)\nfig.update_yaxes(range=[-250, 650])  # y-axis are automatically resized otherwise.\n\nfig.show()"
  }
]